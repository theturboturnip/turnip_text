@misc{amdAMDApproachGPU2017,
  title = {{{AMD}}’s Approach to {{GPU}} Virtualization},
  author = {{Advanced Micro Devices, Inc.} and Knuth, Gabe},
  date = {2017},
  url = {https://www.amd.com/system/files/documents/gpu-consistency-security-whitepaper.pdf},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@report{amdAMDIOVirtualizationTechnology2021,
  title = {{{AMD I}}/{{O Virtualization Technology}} ({{IOMMU}}) {{Specification}}},
  author = {{Advanced Micro Devices, Inc.}},
  date = {2021-04},
  number = {Rev 3.06-PUB},
  url = {https://www.amd.com/system/files/TechDocs/48882_IOMMU.pdf}
}

@online{AMDMakingProgress,
  title = {{{AMD Making Progress On HMM-Based SVM Memory Manager For Open-Source Compute}}},
  url = {https://www.phoronix.com/scan.php?page=news_item&px=AMD-ROCm-HMM-SVM-Memory},
  urldate = {2021-12-27},
  abstract = {This week AMD engineers published their initial code for the AMDGPU/AMDKFD Linux kernel driver for providing a Heterogeneous Memory Management based Shared Virtual Memory (SVM) memory manager that ultimately will be used by their ROCm compute stack.},
  langid = {english}
}

@report{armltdAMBADMAController2009,
  title = {{{AMBA DMA Controller DMA-330 Technical Reference Manual}}},
  author = {{Arm Ltd}},
  date = {2009},
  number = {r1p0},
  url = {https://developer.arm.com/documentation/ddi0424/b},
  urldate = {2021-12-31}
}

@online{armltdARMCortexR8Processor2016,
  title = {{{ARM Cortex-R8 Processor Trail-blazes 5G Need}} for {{Speed}}},
  author = {{Arm Ltd}},
  date = {2016-02-18},
  url = {https://www.arm.com/company/news/2016/02/arm-cortex-r8-processor-trail-blazes-5g-need-for-speed},
  urldate = {2021-12-31},
  abstract = {ARM Cortex-R8 Processor Trail-blazes 5G Need for Speed},
  langid = {english},
  organization = {Arm | The Architecture for the Digital World}
}

@online{armltdArmMaliG78AEProduct2020,
  title = {Arm {{Mali-G78AE Product Brief}}},
  author = {{Arm Ltd}},
  date = {2020},
  url = {https://armkeil.blob.core.windows.net/developer/Files/pdf/product-brief/arm-mali-g78ae-product-brief.pdf},
  urldate = {2021-12-31}
}

@report{armltdArmSMMUVer3,
  title = {Arm {{System Memory Management Unit Architecture Specification}}},
  author = {{Arm Ltd}},
  date = {2021-04-30},
  number = {Version 3},
  url = {https://developer.arm.com/documentation/ihi0070/db/?lang=en},
  urldate = {2021-12-31}
}

@online{armltdCortexR8,
  title = {Cortex-{{R8}}},
  author = {{Arm Ltd}},
  url = {https://developer.arm.com/ip-products/processors/cortex-r/cortex-r8},
  urldate = {2021-12-31},
  abstract = {The Cortex-R8 processor has the highest performance in its class of real-time processors, based on the Armv7-R architecture.},
  langid = {english},
  organization = {Arm Developer}
}

@online{armltdMaliGPUVirtualizationBlog2020,
  title = {Powering Next-Generation in-Vehicle Experiences with {{Arm Mali GPU}} Virtualization},
  author = {{Arm Ltd}},
  date = {2020-06-18},
  url = {https://www.arm.com/company/news/2020/06/powering-next-generation-in-vehicle-experiences},
  urldate = {2021-12-29},
  abstract = {Arm is releasing a new version of its Arm Mali Driver Development Kit (DDK) to support the key requirements of digital cockpit use-cases alongside Mali GPUs.},
  langid = {english}
}

@online{armltdMemoryManagementEmbedded2013,
  title = {Memory {{Management}} on {{Embedded Graphics Processors}} - {{Arm Community}}},
  author = {{Arm Ltd} and Ellis, Sean},
  date = {2013-09-11},
  url = {https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/memory-management-on-embedded-graphics-processors},
  urldate = {2021-12-31},
  abstract = {Our latest world-class embedded graphics processor, the ARM® Mali™-T604 GPU, has excellent memory bandwidth, pixel fill rates to make the mind boggle, and gigaflops of programmable shading power to spare. We need to keep this engine fuelled with data, and since most of its data comes from memory, we have spent a lot of time and effort designing its Memory Management Unit (MMU). I'd like to show you around its headline features, and explain why a properly designed MMU is so important},
  langid = {english}
}

@online{cxlconsortiumComputeExpressLink2021,
  title = {Compute {{Express Link}}™ ({{CXL}}™): {{A Coherent Interface}} for {{Ultra-High-Speed Transfers}}},
  author = {{CXL Consortium}},
  date = {2021-04-16},
  url = {https://www.computeexpresslink.org/resource-library},
  urldate = {2021-12-30}
}

@online{epsrcCAPcelerateCapabilitiesHeterogenous,
  title = {{{CAPcelerate}}: {{Capabilities}} for {{Heterogenous Architectures}}},
  author = {{EPSRC}},
  publisher = {{Engineering and Physical Sciences Research Council, Polaris House, North Star Avenue, Swindon, SN2 1ET}},
  url = {https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/V000381/1},
  urldate = {2021-11-10},
  abstract = {Grants on the web},
  langid = {english}
}

@inproceedings{gomez-lunaChaiCollaborativeHeterogeneous2017,
  title = {Chai: {{Collaborative}} Heterogeneous Applications for Integrated-Architectures},
  shorttitle = {Chai},
  booktitle = {2017 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  author = {Gómez-Luna, Juan and Hajj, Izzat El and Chang, Li-Wen and García-Floreszx, Víctor and family=Gonzalo, given=Simon Garcia, prefix=de, useprefix=true and Jablin, Thomas B. and Peña, Antonio J. and Hwu, Wen-mei},
  date = {2017-04},
  pages = {43--54},
  doi = {10/gnj4dv},
  abstract = {Heterogeneous system architectures are evolving towards tighter integration among devices, with emerging features such as shared virtual memory, memory coherence, and systemwide atomics. Languages, device architectures, system specifications, and applications are rapidly adapting to the challenges and opportunities of tightly integrated heterogeneous platforms. Programming languages such as OpenCL 2.0, CUDA 8.0, and C++ AMP allow programmers to exploit these architectures for productive collaboration between CPU and GPU threads. To evaluate these new architectures and programming languages, and to empower researchers to experiment with new ideas, a suite of benchmarks targeting these architectures with close CPU-GPU collaboration is needed. In this paper, we classify applications that target heterogeneous architectures into generic collaboration patterns including data partitioning, fine-grain task partitioning, and coarse-grain task partitioning. We present Chai, a new suite of 14 benchmarks that cover these patterns and exercise different features of heterogeneous architectures with varying intensity. Each benchmark in Chai has seven different implementations in different programming models such as OpenCL, C++ AMP, and CUDA, and with and without the use of the latest heterogeneous architecture features. We characterize the behavior of each benchmark with respect to varying input sizes and collaboration combinations, and evaluate the impact of using the emerging features of heterogeneous architectures on application performance.},
  eventtitle = {2017 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  keywords = {Benchmark testing,Collaboration,Computer architecture,Graphics processing units,Histograms,Image edge detection,Support vector machines}
}

@unpublished{hubbardGPUsHMMHeterogeneous2017,
  title = {{{GPUs}}: {{HMM}}: {{Heterogeneous Memory Management}}},
  author = {Hubbard, John and Glisse, Jerome},
  date = {2017-05-04},
  url = {https://www.redhat.com/files/summit/session-assets/2017/S104078-hubbard.pdf},
  eventtitle = {Red {{Hat Summit}}},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{ibmpower9nputeamFunctionalityPerformanceNVLink2018,
  title = {Functionality and Performance of {{NVLink}} with {{IBM POWER9}} Processors},
  author = {{IBM POWER9 NPU team}},
  date = {2018-07},
  journaltitle = {IBM Journal of Research and Development},
  volume = {62},
  number = {4/5},
  pages = {9:1-9:10},
  issn = {0018-8646},
  doi = {10/gnxfgt},
  abstract = {Heterogeneous computer systems with multiple types of processing elements (PEs) are becoming a popular design to optimize performance and efficiency for a wide variety of applications. Each part of an application can be executed on the PE for which it is best suited. In heterogeneous systems, communication, efficient data movement, and memory sharing across PEs are critical to execute an application across the different PEs while incurring minimal overhead for communication and synchronization. The IBM POWER9 processor supports the NVIDIA NVLink interface, a high-performance interconnect with many such capabilities. In the IBM Power System AC922, IBM POWER9 processors directly connect to multiple NVIDIA GPUs using NVLink. In this paper, we highlight the important functional and performance capabilities of NVLink with the POWER9 processor. These include high bandwidth, hardware cache coherence, fine-grained data movement, and hardware support for atomic operations across all PEs of a compute node. We also present an analysis of how these performance and functional capabilities of POWER9 processors and NVLink are expected to have significant impacts on performance and programmability across a variety of important applications, such as machine learning and domains within high-performance computing.},
  eventtitle = {{{IBM Journal}} of {{Research}} and {{Development}}},
  keywords = {High performance computing,Program processors}
}

@unpublished{intelAcceleratingPossibilitiesHPC2021,
  title = {Accelerating the {{Possibilities}} with {{HPC}}},
  author = {{Intel} and Damkroger, Trish},
  date = {2021-06-28},
  url = {https://download.intel.com/newsroom/2021/data-center/Intel-ISC2021-keynote-presentation.pdf},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@report{intelIOMMUSpecRev3p3,
  title = {{{Intel}}® {{Virtualization Technology}} for {{Directed I}}/{{O Architecture Specification}}},
  author = {{Intel}},
  date = {2021-04},
  number = {Rev. 3.3},
  pages = {295},
  url = {https://cdrdv2.intel.com/v1/dl/getContent/671081?explicitVersion=true}
}

@online{intelOpenCLSharedVirtual2014,
  title = {{{OpenCL}}™ 2.0 {{Shared Virtual Memory Overview}}},
  author = {{Intel}},
  date = {2014-09-11},
  url = {https://www.intel.com/content/www/us/en/developer/articles/technical/opencl-20-shared-virtual-memory-overview.html},
  urldate = {2021-12-31},
  langid = {english}
}

@report{intelPCISIGSingleRoot2008,
  title = {{{PCI-SIG Single Root I}}/{{O Virtualization}} ({{SR-IOV}}) {{Support}} in {{Intel}}® {{Virtualization Technology}} for {{Connectivity}}},
  author = {{Intel}},
  date = {2008},
  number = {Rev. 06/08-001US},
  pages = {4},
  url = {https://www.intel.com/content/dam/doc/white-paper/pci-sig-single-root-io-virtualization-support-in-virtualization-technology-for-connectivity-paper.pdf},
  abstract = {As virtualized server deployment increases, virtualization technologies continue to evolve especially in the area of I/O performance. Within the industry, significant effort has been expended to increase the effectiveness of hardware resource utilization (i.e., application execution) through the use of virtualization technologies. The Single Root I/O Virtualization and Sharing Specification (SR-IOV) defines extensions to the PCI Express* (PCIe*) specification suite to enable multiple System Images (SI) or Virtual Machines (VMs/Guests) in the virtualized environment to share PCI hardware resources},
  langid = {english}
}

@online{intelUpdatesIntelNextGen2021,
  title = {Updates on {{Intel}}’s {{Next-Gen Data Center Platform}}, {{Sapphire Rapids}}},
  author = {{Intel} and Spelman, Lisa},
  date = {2021-06-29},
  url = {https://www.intel.com/content/www/us/en/newsroom/opinion/updates-next-gen-data-center-platform-sapphire-rapids.html},
  urldate = {2021-12-31},
  abstract = {Rich platform enhancements and built-in acceleration engines will address most demanding workloads from cloud to the edge.},
  langid = {english},
  organization = {Intel}
}

@unpublished{kressinSnapdragon855Deep,
  title = {Snapdragon 855 {{Deep Dives Intro}}},
  author = {Kressin, Keith},
  url = {https://en.wikichip.org/w/images/e/e6/snapdragon-855-deep-dives-intro-keith-kressin.pdf},
  urldate = {2021-12-31}
}

@online{larabelMesa20Nouveau2020,
  title = {Mesa 20.2's {{Nouveau Enables HMM}}, {{OpenCL SVM Now Supported}}},
  author = {Larabel, Michael},
  date = {2020-07-14},
  url = {https://www.phoronix.com/scan.php?page=news_item&px=Mesa-20.2-Nouveau-HMM},
  urldate = {2021-12-31},
  abstract = {The open-source NVIDIA 'Nouveau' driver stack reached a new milestone today with the user-space code in Mesa 20.2 finally flipping on the Heterogeneous Memory Management (HMM) support.},
  langid = {english}
}

@online{larabelRadeonROCmReleased2021,
  title = {Radeon {{ROCm}} 4.3 {{Released With HMM Allocations}}, {{Many Other Improvements}}},
  author = {Larabel, Michael},
  date = {2021-08-03},
  url = {https://www.phoronix.com/scan.php?page=news_item&px=Radeon-ROCm-4.3},
  urldate = {2021-12-31},
  abstract = {AMD has released ROCm 4.3 as the newest version of their Radeon Open eCosystem stack for providing open-source GPU compute and CUDA portability for their supported graphics processors under Linux},
  langid = {english}
}

@article{liEvaluatingModernGPU2020,
  title = {Evaluating {{Modern GPU Interconnect}}: {{PCIe}}, {{NVLink}}, {{NV-SLI}}, {{NVSwitch}} and {{GPUDirect}}},
  shorttitle = {Evaluating {{Modern GPU Interconnect}}},
  author = {Li, Ang and Song, Shuaiwen Leon and Chen, Jieyang and Li, Jiajia and Liu, Xu and Tallent, Nathan and Barker, Kevin},
  date = {2020-01-01},
  journaltitle = {IEEE Transactions on Parallel and Distributed Systems},
  shortjournal = {IEEE Trans. Parallel Distrib. Syst.},
  volume = {31},
  number = {1},
  eprint = {1903.04611},
  eprinttype = {arxiv},
  pages = {94--110},
  issn = {1045-9219, 1558-2183, 2161-9883},
  doi = {10/ggqsjd},
  url = {http://arxiv.org/abs/1903.04611},
  urldate = {2021-12-23},
  abstract = {High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Hardware Architecture,Computer Science - Networking and Internet Architecture,Computer Science - Performance}
}

@online{linuxkernelAMDgpuDriver,
  title = {Drm/Amdgpu {{AMDgpu}} Driver — {{The Linux Kernel}} Documentation},
  author = {{Linux Kernel Dev Community}},
  url = {https://www.kernel.org/doc/html/v5.0/gpu/amdgpu.html},
  urldate = {2021-12-27}
}

@online{linuxkernelHeterogeneousMemoryManagement,
  title = {Heterogeneous {{Memory Management}} ({{HMM}}) — {{The Linux Kernel}} Documentation},
  author = {Linux Kernel Dev Community},
  url = {https://www.kernel.org/doc/html/v5.11/vm/hmm.html},
  urldate = {2021-12-31}
}

@online{linuxkernelLinuxAndTheDevicetree,
  title = {Linux and the {{Devicetree}} — {{The Linux Kernel}} Documentation},
  author = {{Linux Kernel Dev Community} and Likely, Grant},
  url = {https://www.kernel.org/doc/html/latest/devicetree/usage-model.html},
  urldate = {2021-12-31}
}

@online{LVC20309ArmSMMU,
  title = {{{LVC20-309 The Arm SMMU}} and the {{Adreno GPU}}},
  url = {https://connect.linaro.org/resources/lvc20/lvc20-309/},
  urldate = {2021-12-29},
  abstract = {The Qualcomm Adreno GPU pushes the boundaries of the ARM SMMUv2 architecture in new and interesting ways. This presentation will discuss some of the new proposed GPU specific features for the ARM SMMUv2 driver such as split pagetables and pagetable switching and future enhancements to improve the GPU/SMMU relationship.},
  langid = {english},
  organization = {Linaro}
}

@inproceedings{markettosPositionPaperDefending2020,
  title = {Position {{Paper}}:{{Defending Direct Memory Access}} with {{CHERI Capabilities}}},
  shorttitle = {Position {{Paper}}},
  booktitle = {Hardware and {{Architectural Support}} for {{Security}} and {{Privacy}}},
  author = {Markettos, A. Theodore and Baldwin, John and Bukin, Ruslan and Neumann, Peter G. and Moore, Simon W. and Watson, Robert N. M.},
  date = {2020-10-17},
  pages = {1--9},
  publisher = {ACM},
  location = {Virtual Greece},
  doi = {10/gnckmg},
  url = {https://dl.acm.org/doi/10.1145/3458903.3458910},
  urldate = {2021-12-30},
  abstract = {We propose new solutions that can efficiently address the problem of malicious memory access from pluggable computer peripherals and microcontrollers embedded within a system-on-chip. This problem represents a serious emerging threat to total-system computer security. Previous work has shown that existing defenses are insufficient and poorly deployed, in part due to performance concerns. In this paper we explore the threat and its implications for system architecture. We propose a range of protection techniques, from lightweight to heavyweight, across different classes of systems. We consider how emerging capability architectures (and specifically the CHERI protection model) can enhance protection and provide a convenient bridge to describe interactions among software and hardware components. Finally, we describe how new schemes may be more efficient than existing defenses.},
  eventtitle = {{{HASP}} '20: {{Hardware}} and {{Architectural Support}} for {{Security}} and {{Privacy}}},
  isbn = {978-1-4503-8898-6},
  langid = {english}
}

@inproceedings{markettosThunderclapExploringVulnerabilities2019,
  title = {Thunderclap: {{Exploring Vulnerabilities}} in {{Operating System IOMMU Protection}} via {{DMA}} from {{Untrustworthy Peripherals}}},
  shorttitle = {Thunderclap},
  booktitle = {Proceedings 2019 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Markettos, A. Theodore and Rothwell, Colin and Gutstein, Brett F. and Pearce, Allison and Neumann, Peter G. and Moore, Simon W. and Watson, Robert N. M.},
  date = {2019},
  publisher = {Internet Society},
  location = {San Diego, CA},
  doi = {10/gjh62d},
  url = {https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_05A-1_Markettos_paper.pdf},
  urldate = {2021-12-31},
  abstract = {Direct Memory Access (DMA) attacks have been known for many years: DMA-enabled I/O peripherals have complete access to the state of a computer and can fully compromise it including reading and writing all of system memory. With the popularity of Thunderbolt 3 over USB Type-C and smart internal devices, opportunities for these attacks to be performed casually with only seconds of physical access to a computer have greatly broadened. In response, commodity hardware and operatingsystem (OS) vendors have incorporated support for Input-Ouptut Memory Management Units (IOMMUs), which impose memory protection on DMA, and are widely believed to protect against DMA attacks. We investigate the state-of-the-art in IOMMU protection across OSes using a novel I/O-security research platform, and find that current protections fall short when faced with a functional network peripheral that uses its complex interactions with the OS for ill intent. We describe vulnerabilities in macOS, FreeBSD, and Linux, which notionally utilize IOMMUs to protect against DMA attackers. Windows uses the IOMMU only in limited cases. and it remains vulnerable. Using Thunderclap, an open-source FPGA research platform that we built, we explore new classes of OS vulnerability arising from inadequate use of the IOMMU. The complex vulnerability space for IOMMUexposed shared memory available to DMA-enabled peripherals allows attackers to extract private data (sniffing cleartext VPN traffic) and hijack kernel control flow (launching a root shell) in seconds using devices such as USB-C projectors or power adapters. We have worked closely with OS vendors to remedy these vulnerability classes, and they have now shipped substantial feature improvements and mitigations as a result of our work.},
  eventtitle = {Network and {{Distributed System Security Symposium}}},
  isbn = {978-1-891562-55-6},
  langid = {english}
}

@inproceedings{markuzeTrueIOMMUProtection2016,
  title = {True {{IOMMU Protection}} from {{DMA Attacks}}: {{When Copy}} Is {{Faster}} than {{Zero Copy}}},
  shorttitle = {True {{IOMMU Protection}} from {{DMA Attacks}}},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Markuze, Alex and Morrison, Adam and Tsafrir, Dan},
  date = {2016-03-25},
  series = {{{ASPLOS}} '16},
  pages = {249--262},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2872362.2872379},
  url = {https://dl.acm.org/doi/10.1145/2872362.2872379},
  urldate = {2023-10-02},
  abstract = {Malicious I/O devices might compromise the OS using DMAs. The OS therefore utilizes the IOMMU to map and unmap every target buffer right before and after its DMA is processed, thereby restricting DMAs to their designated locations. This usage model, however, is not truly secure for two reasons: (1) it provides protection at page granularity only, whereas DMA buffers can reside on the same page as other data; and (2) it delays DMA buffer unmaps due to performance considerations, creating a vulnerability window in which devices can access in-use memory. We propose that OSes utilize the IOMMU differently, in a manner that eliminates these two flaws. Our new usage model restricts device access to a set of shadow DMA buffers that are never unmapped, and it copies DMAed data to/from these buffers, thus providing sub-page protection while eliminating the aforementioned vulnerability window. Our key insight is that the cost of interacting with, and synchronizing access to the slow IOMMU hardware---required for zero-copy protection against devices---make copying preferable to zero-copying. We implement our model in Linux and evaluate it with standard networking benchmarks utilizing a 40,Gb/s NIC. We demonstrate that despite being more secure than the safest preexisting usage model, our approach provides up to 5x higher throughput. Additionally, whereas it is inherently less scalable than an IOMMU-less (unprotected) system, our approach incurs only 0\%--25\% performance degradation in comparison.},
  isbn = {978-1-4503-4091-5},
  keywords = {dma attacks,IOMMU}
}

@unpublished{minEMOGIEfficientMemoryaccess2021,
  title = {{{EMOGI}}: {{Efficient Memory-access}} for {{Out-of-memory Graph-traversal In GPUs}}},
  shorttitle = {{{EMOGI}}},
  author = {Min, Seung Won and Mailthody, Vikram Sharma and Qureshi, Zaid and Xiong, Jinjun and Ebrahimi, Eiman and Hwu, Wen-mei},
  date = {2021-01-14},
  eprint = {2006.06890},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2006.06890},
  urldate = {2021-11-08},
  abstract = {Modern analytics and recommendation systems are increasingly based on graph data that capture the relations between entities being analyzed. Practical graphs come in huge sizes, offer massive parallelism, and are stored in sparse-matrix formats such as compressed sparse row (CSR). To exploit the massive parallelism, developers are increasingly interested in using GPUs for graph traversal. However, due to their sizes, graphs often do not fit into the GPU memory. Prior works have either used input data preprocessing/partitioning or unified virtual memory (UVM) to migrate chunks of data from the host memory to the GPU memory. However, the large, multi-dimensional, and sparse nature of graph data presents a major challenge to these schemes and results in significant amplification of data movement and reduced effective data throughput. In this work, we propose EMOGI, an alternative approach to traverse graphs that do not fit in GPU memory using direct cache-line-sized access to data stored in host memory.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Databases,Computer Science - Distributed Parallel and Cluster Computing}
}

@article{morganIOMMUProtectionAttacks2018,
  title = {{{IOMMU}} Protection against {{I}}/{{O}} Attacks: A Vulnerability and a Proof of Concept},
  shorttitle = {{{IOMMU}} Protection against {{I}}/{{O}} Attacks},
  author = {Morgan, Benoît and Alata, Éric and Nicomette, Vincent and Kaâniche, Mohamed},
  date = {2018-01-09},
  journaltitle = {Journal of the Brazilian Computer Society},
  shortjournal = {Journal of the Brazilian Computer Society},
  volume = {24},
  number = {1},
  pages = {2},
  issn = {1678-4804},
  doi = {10/gf8xn6},
  url = {https://doi.org/10.1186/s13173-017-0066-7},
  urldate = {2021-12-29},
  abstract = {Input/output (I/O) attacks have received increasing attention during the last decade. These attacks are performed by malicious peripherals that make read or write accesses to DRAM memory or to memory embedded in other peripherals, through DMA (Direct Memory Access) requests. Some protection mechanisms have been implemented in modern architectures to face these attacks. A typical example is the IOMMU (Input-Output Memory Management Unit). However, such mechanisms may not be properly configured and used by the firmware and the operating system. This paper describes a design weakness that we discovered in the configuration of an IOMMU and a possible exploitation scenario that would allow a malicious peripheral to bypass the underlying protection mechanism. The exploitation scenario is implemented for Intel architectures, with a PCI Express peripheral Field Programmable Gate Array, based on Intel specifications and Linux source code analysis. Finally, as a proof of concept, a Linux rootkit based on the attack presented in this paper is implemented.},
  keywords = {Attack,Firmware,IOMMU,Linux,Security,Vulnerability}
}

@misc{nvidiacorporationCUDAProgrammingGuide,
  title = {{{CUDA C}}++ {{Programming Guide}} V11.5.0},
  author = {{NVIDIA Corporation}},
  url = {https://docs.nvidia.com/cuda/archive/11.5.0/cuda-c-programming-guide/index.html},
  urldate = {2021-11-11},
  abstract = {The programming guide to the CUDA model and interface.},
  langid = {american}
}

@online{nvidiacorporationNVIDIAJetsonTX22017,
  title = {{{NVIDIA Jetson TX2 Delivers Twice}} the {{Intelligence}} to the {{Edge}}},
  author = {{NVIDIA Corporation}},
  date = {2017-03-08},
  url = {https://developer.nvidia.com/blog/jetson-tx2-delivers-twice-intelligence-edge/},
  urldate = {2021-12-31},
  abstract = {Today at an AI meetup in San Francisco, NVIDIA launched Jetson TX2 and the JetPack 3.0 AI SDK. Jetson is the world’s leading low-power embedded platform, enabling server-class AI compute performance…},
  langid = {american},
  organization = {NVIDIA Developer Blog}
}

@report{nvidiacorporationNVIDIAMultiInstanceGPU2020,
  title = {{{NVIDIA Multi-Instance GPU}} and {{NVIDIA}}  {{Virtual Compute Server}} - {{Technical Brief}}},
  author = {{NVIDIA Corporation}},
  date = {2020-11},
  number = {TB-10226-001\_v01},
  url = {https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/solutions/resources/documents1/TB-10226-001_v01.pdf},
  urldate = {2021-12-27}
}

@online{nvidiacorporationPascalMMUFormat2016,
  title = {Pascal {{MMU Format Changes}} - {{OpenGPUDoc}}},
  author = {{NVIDIA Corporation}},
  date = {2016-11-22},
  url = {https://nvidia.github.io/open-gpu-doc/pascal/gp100-mmu-format.pdf},
  urldate = {2021-12-27}
}

@unpublished{nvidiacorporationS8868CUDAXavier2018,
  title = {S8868 - {{CUDA}} on {{Xavier}}},
  author = {{NVIDIA Corporation} and Bhat, Anshuman and Dasadhikari, Saikat},
  date = {2018-03-29},
  url = {https://on-demand.gputechconf.com/gtc/2018/presentation/s8868-cuda-on-xavier-what-is-new.pdf},
  urldate = {2021-12-31},
  venue = {GTC 2018}
}

@online{nvidiacorporationUnifiedMemoryCUDA2017,
  title = {Unified {{Memory}} for {{CUDA Beginners}}},
  author = {{NVIDIA Corporation} and Harris, Mark},
  date = {2017-06-20},
  url = {https://developer.nvidia.com/blog/unified-memory-cuda-beginners/},
  urldate = {2021-12-31},
  abstract = {This post introduces CUDA programming with Unified Memory, a single memory address space that is accessible from any GPU or CPU in a system.},
  langid = {american},
  organization = {NVIDIA Developer Blog}
}

@report{PCIeATSR1.1,
  title = {Address {{Translation Services}}},
  author = {{PCI-SIG}},
  date = {2009-01-26},
  number = {Revision 1.1},
  pages = {54},
  url = {https://composter.com.ua/documents/ats_r1.1_26Jan09.pdf},
  langid = {english}
}

@online{qualcommSnapdragonGenMobile2021,
  title = {Snapdragon 8 {{Gen}} 1 {{Mobile Platform}}},
  author = {{Qualcomm}},
  date = {2021-11-17T12:48:06-08:00},
  url = {https://www.qualcomm.com/products/snapdragon-8-gen-1-mobile-platform},
  urldate = {2021-12-31},
  abstract = {Our most advanced 5G platform ever with ground-breaking innovations in AI, photography, gaming, and connectivity.},
  langid = {english}
}

@online{samsungExynos980Release,
  title = {Samsung {{Introduces}} Its {{First 5G-Integrated Mobile Processor}}, the {{Exynos}} 980},
  author = {{Samsung Semiconductor}},
  url = {https://www.samsung.com/semiconductor/minisite/exynos/newsroom/pressrelease/samsung-introduces-its-first-5g-integrated-mobile-processor-the-exynos-980/},
  urldate = {2021-12-31},
  abstract = {With multi-mode capabilities and sub-6GHz speed at up to 2.55Gbps, the Exynos 980 will help bring 5G connectivity to more mobile devices},
  langid = {english}
}

@inproceedings{suzukiGPUvmWhyNot2014,
  title = {{{GPUvm}}: {{Why Not Virtualizing GPUs}} at the {{Hypervisor}}?},
  shorttitle = {{{GPUvm}}},
  author = {Suzuki, Yusuke and Kato, Shinpei and Yamada, Hiroshi and Kono, Kenji},
  date = {2014},
  pages = {109--120},
  url = {https://www.usenix.org/conference/atc14/technical-sessions/presentation/suzuki},
  urldate = {2021-12-27},
  eventtitle = {2014 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 14)},
  isbn = {978-1-931971-10-2},
  langid = {english}
}

@inproceedings{tineVortexExtendingRISCV2021,
  title = {Vortex: {{Extending}} the {{RISC-V ISA}} for {{GPGPU}} and {{3D-Graphics}}},
  shorttitle = {Vortex},
  booktitle = {{{MICRO-54}}: 54th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Tine, Blaise and Yalamarthy, Krishna Praveen and Elsabbagh, Fares and Hyesoon, Kim},
  date = {2021-10-18},
  pages = {754--766},
  publisher = {ACM},
  location = {Virtual Event Greece},
  doi = {10/gnj4cn},
  url = {https://dl.acm.org/doi/10.1145/3466752.3480128},
  urldate = {2021-11-22},
  abstract = {The importance of open-source hardware and software has been increasing. However, despite GPUs being one of the more popular accelerators across various applications, there is very little opensource GPU infrastructure in the public domain. We argue that one of the reasons for the lack of open-source infrastructure for GPUs is rooted in the complexity of their ISA and software stacks. In this work, we first propose an ISA extension to RISC-V that supports GPGPUs and graphics. The main goal of the ISA extension proposal is to minimize the ISA changes so that the corresponding changes to the open-source ecosystem are also minimal, which makes for a sustainable development ecosystem. To demonstrate the feasibility of the minimally extended RISC-V ISA, we implemented the complete software and hardware stacks of Vortex on FPGA. Vortex is a PCIe-based soft GPU that supports OpenCL and OpenGL. Vortex can be used in a variety of applications, including machine learning, graph analytics, and graphics rendering. Vortex can scale up to 32 cores on an Altera Stratix 10 FPGA, delivering a peak performance of 25.6 GFlops at 200 Mhz.},
  eventtitle = {{{MICRO}} '21: 54th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  isbn = {978-1-4503-8557-2},
  langid = {english}
}

@online{UnderstandingIommuLinux,
  title = {Understanding the Iommu {{Linux}} Grub {{File Configuration}}},
  url = {https://community.mellanox.com/s/article/understanding-the-iommu-linux-grub-file-configuration},
  urldate = {2021-12-27}
}

@article{wangGrusUnifiedmemoryefficientHighperformance2021,
  title = {Grus: {{Toward Unified-memory-efficient High-performance Graph Processing}} on {{GPU}}},
  shorttitle = {Grus},
  author = {Wang, Pengyu and Wang, Jing and Li, Chao and Wang, Jianzong and Zhu, Haojin and Guo, Minyi},
  date = {2021-02-09},
  journaltitle = {ACM Transactions on Architecture and Code Optimization},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  volume = {18},
  number = {2},
  pages = {22:1--22:25},
  issn = {1544-3566},
  doi = {10/gnpp58},
  url = {https://doi.org/10.1145/3444844},
  urldate = {2021-12-05},
  abstract = {Today’s GPU graph processing frameworks face scalability and efficiency issues as the graph size exceeds GPU-dedicated memory limit. Although recent GPUs can over-subscribe memory with Unified Memory (UM), they incur significant overhead when handling graph-structured data. In addition, many popular processing frameworks suffer sub-optimal efficiency due to heavy atomic operations when tracking the active vertices. This article presents Grus, a novel system framework that allows GPU graph processing to stay competitive with the ever-growing graph complexity. Grus improves space efficiency through a UM trimming scheme tailored to the data access behaviors of graph workloads. It also uses a lightweight frontier structure to further reduce atomic operations. With easy-to-use interface that abstracts the above details, Grus shows up to 6.4× average speedup over the state-of-the-art in-memory GPU graph processing framework. It allows one to process large graphs of 5.5 billion edges in seconds with a single GPU.},
  keywords = {GPU,Graph processing,unified virtual memory}
}

@inproceedings{watsonCHERIHybridCapabilitySystem2015,
  title = {{{CHERI}}: {{A Hybrid Capability-System Architecture}} for {{Scalable Software Compartmentalization}}},
  shorttitle = {{{CHERI}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Watson, Robert N.M. and Woodruff, Jonathan and Neumann, Peter G. and Moore, Simon W. and Anderson, Jonathan and Chisnall, David and Dave, Nirav and Davis, Brooks and Gudka, Khilan and Laurie, Ben and Murdoch, Steven J. and Norton, Robert and Roe, Michael and Son, Stacey and Vadera, Munraj},
  date = {2015-05},
  pages = {20--37},
  publisher = {IEEE},
  location = {San Jose, CA},
  doi = {10/gfpgzz},
  url = {https://ieeexplore.ieee.org/document/7163016/},
  urldate = {2021-12-30},
  abstract = {CHERI extends a conventional RISC InstructionSet Architecture, compiler, and operating system to support fine-grained, capability-based memory protection to mitigate memory-related vulnerabilities in C-language TCBs. We describe how CHERI capabilities can also underpin a hardware-software object-capability model for application compartmentalization that can mitigate broader classes of attack. Prototyped as an extension to the open-source 64-bit BERI RISC FPGA softcore processor, FreeBSD operating system, and LLVM compiler, we demonstrate multiple orders-of-magnitude improvement in scalability, simplified programmability, and resulting tangible security benefits as compared to compartmentalization based on pure Memory-Management Unit (MMU) designs. We evaluate incrementally deployable CHERI-based compartmentalization using several real-world UNIX libraries and applications.},
  eventtitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  isbn = {978-1-4673-6949-7},
  langid = {english}
}

@inproceedings{xiangExploitingUniformVector2013,
  title = {Exploiting Uniform Vector Instructions for {{GPGPU}} Performance, Energy Efficiency, and Opportunistic Reliability Enhancement},
  booktitle = {Proceedings of the 27th International {{ACM}} Conference on {{International}} Conference on Supercomputing},
  author = {Xiang, Ping and Yang, Yi and Mantor, Mike and Rubin, Norm and Hsu, Lisa R. and Zhou, Huiyang},
  date = {2013-06-10},
  series = {{{ICS}} '13},
  pages = {433--442},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10/gnj4ds},
  url = {https://doi.org/10.1145/2464996.2465022},
  urldate = {2021-11-22},
  abstract = {State-of-art graphics processing units (GPUs) employ the single-instruction multiple-data (SIMD) style execution to achieve both high computational throughput and energy efficiency. As previous works have shown, there exists significant computational redundancy in SIMD execution, where different execution lanes operate on the same operand values. Such value locality is referred to as uniform vectors. In this paper, we first show that besides redundancy within a uniform vector, different vectors can also have the identical values. Then, we propose detailed architecture designs to exploit both types of redundancy. For redundancy within a uniform vector, we propose to either extend the vector register file with token bits or add a separate small scalar register file to eliminate redundant computations as well as redundant data storage. For redundancy across different uniform vectors, we adopt instruction reuse, proposed originally for CPU architectures, to detect and eliminate redundancy. The elimination of redundant computations and data storage leads to both significant energy savings and performance improvement. Furthermore, we propose to leverage such redundancy to protect arithmetic-logic units (ALUs) and register files against hardware errors. Our detailed evaluation shows that our proposed design has low hardware overhead and achieves performance gains, up to 23.9\% and 12.0\% on average, along with energy savings, up to 24.8\% and 12.6\% on average, as well as a 21.1\% and 14.1\% protection coverage for ALUs and register files, respectively.},
  isbn = {978-1-4503-2130-3},
  keywords = {GPGPU,redundancy}
}

@article{yuArchitectureSupportedRegister2016,
  title = {Architecture Supported Register Stash for {{GPGPU}}},
  author = {Yu, Licheng and Pei, Yulong and Chen, Tianzhou and Wu, Minghui},
  date = {2016-03-01},
  journaltitle = {Journal of Parallel and Distributed Computing},
  shortjournal = {J. Parallel Distrib. Comput.},
  volume = {89},
  number = {C},
  pages = {25--36},
  issn = {0743-7315},
  doi = {10/f8cwmn},
  url = {https://doi.org/10.1016/j.jpdc.2015.12.003},
  urldate = {2021-11-22},
  abstract = {GPGPU provides abundant hardware resources to support a large number of light-weighted threads. They are organized into blocks and run in warps. All threads of a block must be dispatched to one stream multiprocessor (SM) of GPGPU together. When the remaining resources of an SM cannot support one more block, all threads of the block are held back until former blocks retire from the SM. We found that the register file is prone to be the most limited one among all the resources, especially for SMs with less registers. Meanwhile, we revealed the dynamics of a thread's register requirement: only part of its pre-allocated registers are used for different instructions at run time. This results in considerable register underutilization.We proposed the architecture supported register stash (ASRS). It removes the limitation of registers when dispatching blocks. The hardware registers are allocated at run time according to each instruction's live registers, which can be analyzed statically by a compiler. When the hardware registers cannot meet the requirements of all running warps, some warps are suspended and their registers are reclaimed temporarily. The data in these registers are stashed to memory. On the other hand, if there are spare hardware registers, it will start a new warp or resume a suspended warp after all the warp's stashed register data are loaded from memory. The intra-block synchronization is also taken care of when some of the warps of the same block are not schedulable due to the ASRS.The ASRS alleviates the register underutilization and improves performance without modifying the current programming model or demanding extra effort from the programmers. It also enables an SM with limited registers that cannot even support a single block to execute it. Besides, it helps lower the register file energy consumption and increase the power efficiency. The ASRS achieved speedups of 1.59 and 1.14 when the registers of each SM are limited to 8K and 16K respectively with an insignificant overhead. The speedups compared with the infinite register files are 0.84 and 0.98 with 8K and 16K registers respectively. Compared with the baseline 32K register file, the ASRS decreases the 8K and 16K register file energy consumption to 66.5\% and 75.8\% respectively. Their power efficiencies (in ratio of performance and power) are increased to 1.29x and 1.31x respectively. Register requirement of GPGPU varies among different kernels and during run time.Register file (RF) capacity limits the schedulable warps and performance.Reducing RF capacity lowers energy consumption and area.We proposed a method to support more warps with limited registers.It gains significant speedup and a higher energy efficiency with a smaller RF.},
  keywords = {Energy,GPGPU,Performance,Register file}
}
