[
  {"id":"amdAMDApproachGPU2017","author":[{"literal":"Advanced Micro Devices, Inc."},{"family":"Knuth","given":"Gabe"}],"citation-key":"amdAMDApproachGPU2017","issued":{"date-parts":[["2017"]]},"language":"en","source":"Zotero","title":"AMD’s approach to GPU virtualization","type":"document","URL":"https://www.amd.com/system/files/documents/gpu-consistency-security-whitepaper.pdf"},
  {"id":"amdAMDIOVirtualizationTechnology2021","author":[{"literal":"Advanced Micro Devices, Inc."}],"citation-key":"amdAMDIOVirtualizationTechnology2021","issued":{"date-parts":[["2021",4]]},"number":"Rev 3.06-PUB","title":"AMD I/O Virtualization Technology (IOMMU) Specification","type":"report","URL":"https://www.amd.com/system/files/TechDocs/48882_IOMMU.pdf"},
  {"id":"AMDMakingProgress","abstract":"This week AMD engineers published their initial code for the AMDGPU/AMDKFD Linux kernel driver for providing a Heterogeneous Memory Management based Shared Virtual Memory (SVM) memory manager that ultimately will be used by their ROCm compute stack.","accessed":{"date-parts":[["2021",12,27]]},"citation-key":"AMDMakingProgress","language":"en","title":"AMD Making Progress On HMM-Based SVM Memory Manager For Open-Source Compute","type":"webpage","URL":"https://www.phoronix.com/scan.php?page=news_item&px=AMD-ROCm-HMM-SVM-Memory"},
  {"id":"armltdAMBADMAController2009","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Arm Ltd"}],"citation-key":"armltdAMBADMAController2009","issued":{"date-parts":[["2009"]]},"number":"r1p0","title":"AMBA DMA Controller DMA-330 Technical Reference Manual","type":"report","URL":"https://developer.arm.com/documentation/ddi0424/b"},
  {"id":"armltdARMCortexR8Processor2016","abstract":"ARM Cortex-R8 Processor Trail-blazes 5G Need for Speed","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Arm Ltd"}],"citation-key":"armltdARMCortexR8Processor2016","container-title":"Arm | The Architecture for the Digital World","issued":{"date-parts":[["2016",2,18]]},"language":"en","title":"ARM Cortex-R8 Processor Trail-blazes 5G Need for Speed","type":"webpage","URL":"https://www.arm.com/company/news/2016/02/arm-cortex-r8-processor-trail-blazes-5g-need-for-speed"},
  {"id":"armltdArmMaliG78AEProduct2020","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Arm Ltd"}],"citation-key":"armltdArmMaliG78AEProduct2020","issued":{"date-parts":[["2020"]]},"title":"Arm Mali-G78AE Product Brief","type":"webpage","URL":"https://armkeil.blob.core.windows.net/developer/Files/pdf/product-brief/arm-mali-g78ae-product-brief.pdf"},
  {"id":"armltdArmSMMUVer3","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Arm Ltd"}],"citation-key":"armltdArmSMMUVer3","issued":{"date-parts":[["2021",4,30]]},"number":"Version 3","title":"Arm System Memory Management Unit Architecture Specification","type":"report","URL":"https://developer.arm.com/documentation/ihi0070/db/?lang=en"},
  {"id":"armltdCortexR8","abstract":"The Cortex-R8 processor has the highest performance in its class of real-time processors, based on the Armv7-R architecture.","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Arm Ltd"}],"citation-key":"armltdCortexR8","container-title":"Arm Developer","language":"en","title":"Cortex-R8","type":"webpage","URL":"https://developer.arm.com/ip-products/processors/cortex-r/cortex-r8"},
  {"id":"armltdMaliGPUVirtualizationBlog2020","abstract":"Arm is releasing a new version of its Arm Mali Driver Development Kit (DDK) to support the key requirements of digital cockpit use-cases alongside Mali GPUs.","accessed":{"date-parts":[["2021",12,29]]},"author":[{"literal":"Arm Ltd"}],"citation-key":"armltdMaliGPUVirtualizationBlog2020","issued":{"date-parts":[["2020",6,18]]},"language":"en","title":"Powering next-generation in-vehicle experiences with Arm Mali GPU virtualization","type":"webpage","URL":"https://www.arm.com/company/news/2020/06/powering-next-generation-in-vehicle-experiences"},
  {"id":"armltdMemoryManagementEmbedded2013","abstract":"Our latest world-class embedded graphics processor, the ARM® Mali™-T604 GPU, has excellent memory bandwidth, pixel fill rates to make the mind boggle, and gigaflops of programmable shading power to spare.\n\nWe need to keep this engine fuelled with data, and since most of its data comes from memory, we have spent a lot of time and effort designing its Memory Management Unit (MMU). I'd like to show you around its headline features, and explain why a properly designed MMU is so important","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Arm Ltd"},{"family":"Ellis","given":"Sean"}],"citation-key":"armltdMemoryManagementEmbedded2013","issued":{"date-parts":[["2013",9,11]]},"language":"en","title":"Memory Management on Embedded Graphics Processors - Arm Community","type":"webpage","URL":"https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/memory-management-on-embedded-graphics-processors"},
  {"id":"cxlconsortiumComputeExpressLink2021","accessed":{"date-parts":[["2021",12,30]]},"author":[{"literal":"CXL Consortium"}],"citation-key":"cxlconsortiumComputeExpressLink2021","issued":{"date-parts":[["2021",4,16]]},"title":"Compute Express Link™ (CXL™): A Coherent Interface for Ultra-High-Speed Transfers","type":"webpage","URL":"https://www.computeexpresslink.org/resource-library"},
  {"id":"epsrcCAPcelerateCapabilitiesHeterogenous","abstract":"Grants on the web","accessed":{"date-parts":[["2021",11,10]]},"author":[{"literal":"EPSRC"}],"citation-key":"epsrcCAPcelerateCapabilitiesHeterogenous","language":"en","publisher":"Engineering and Physical Sciences Research Council, Polaris House, North Star Avenue, Swindon, SN2 1ET","title":"CAPcelerate: Capabilities for Heterogenous Architectures","type":"webpage","URL":"https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/V000381/1"},
  {"id":"gomez-lunaChaiCollaborativeHeterogeneous2017","abstract":"Heterogeneous system architectures are evolving towards tighter integration among devices, with emerging features such as shared virtual memory, memory coherence, and systemwide atomics. Languages, device architectures, system specifications, and applications are rapidly adapting to the challenges and opportunities of tightly integrated heterogeneous platforms. Programming languages such as OpenCL 2.0, CUDA 8.0, and C++ AMP allow programmers to exploit these architectures for productive collaboration between CPU and GPU threads. To evaluate these new architectures and programming languages, and to empower researchers to experiment with new ideas, a suite of benchmarks targeting these architectures with close CPU-GPU collaboration is needed. In this paper, we classify applications that target heterogeneous architectures into generic collaboration patterns including data partitioning, fine-grain task partitioning, and coarse-grain task partitioning. We present Chai, a new suite of 14 benchmarks that cover these patterns and exercise different features of heterogeneous architectures with varying intensity. Each benchmark in Chai has seven different implementations in different programming models such as OpenCL, C++ AMP, and CUDA, and with and without the use of the latest heterogeneous architecture features. We characterize the behavior of each benchmark with respect to varying input sizes and collaboration combinations, and evaluate the impact of using the emerging features of heterogeneous architectures on application performance.","author":[{"family":"Gómez-Luna","given":"Juan"},{"family":"Hajj","given":"Izzat El"},{"family":"Chang","given":"Li-Wen"},{"family":"García-Floreszx","given":"Víctor"},{"family":"Gonzalo","given":"Simon Garcia","non-dropping-particle":"de"},{"family":"Jablin","given":"Thomas B."},{"family":"Peña","given":"Antonio J."},{"family":"Hwu","given":"Wen-mei"}],"citation-key":"gomez-lunaChaiCollaborativeHeterogeneous2017","container-title":"2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","DOI":"10/gnj4dv","event-title":"2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","issued":{"date-parts":[["2017",4]]},"page":"43-54","source":"IEEE Xplore","title":"Chai: Collaborative heterogeneous applications for integrated-architectures","title-short":"Chai","type":"paper-conference"},
  {"id":"hubbardGPUsHMMHeterogeneous2017","author":[{"family":"Hubbard","given":"John"},{"family":"Glisse","given":"Jerome"}],"citation-key":"hubbardGPUsHMMHeterogeneous2017","event-title":"Red Hat Summit","issued":{"date-parts":[["2017",5,4]]},"language":"en","title":"GPUs: HMM: Heterogeneous Memory Management","type":"speech","URL":"https://www.redhat.com/files/summit/session-assets/2017/S104078-hubbard.pdf"},
  {"id":"ibmpower9nputeamFunctionalityPerformanceNVLink2018","abstract":"Heterogeneous computer systems with multiple types of processing elements (PEs) are becoming a popular design to optimize performance and efficiency for a wide variety of applications. Each part of an application can be executed on the PE for which it is best suited. In heterogeneous systems, communication, efficient data movement, and memory sharing across PEs are critical to execute an application across the different PEs while incurring minimal overhead for communication and synchronization. The IBM POWER9 processor supports the NVIDIA NVLink interface, a high-performance interconnect with many such capabilities. In the IBM Power System AC922, IBM POWER9 processors directly connect to multiple NVIDIA GPUs using NVLink. In this paper, we highlight the important functional and performance capabilities of NVLink with the POWER9 processor. These include high bandwidth, hardware cache coherence, fine-grained data movement, and hardware support for atomic operations across all PEs of a compute node. We also present an analysis of how these performance and functional capabilities of POWER9 processors and NVLink are expected to have significant impacts on performance and programmability across a variety of important applications, such as machine learning and domains within high-performance computing.","author":[{"literal":"IBM POWER9 NPU team"}],"citation-key":"ibmpower9nputeamFunctionalityPerformanceNVLink2018","container-title":"IBM Journal of Research and Development","DOI":"10/gnxfgt","ISSN":"0018-8646","issue":"4/5","issued":{"date-parts":[["2018",7]]},"page":"9:1-9:10","source":"IEEE Xplore","title":"Functionality and performance of NVLink with IBM POWER9 processors","type":"article-journal","volume":"62"},
  {"id":"intelAcceleratingPossibilitiesHPC2021","author":[{"literal":"Intel"},{"family":"Damkroger","given":"Trish"}],"citation-key":"intelAcceleratingPossibilitiesHPC2021","issued":{"date-parts":[["2021",6,28]]},"language":"en","title":"Accelerating the Possibilities with HPC","type":"speech","URL":"https://download.intel.com/newsroom/2021/data-center/Intel-ISC2021-keynote-presentation.pdf"},
  {"id":"intelIOMMUSpecRev3p3","author":[{"literal":"Intel"}],"citation-key":"intelIOMMUSpecRev3p3","issued":{"date-parts":[["2021",4]]},"number":"Rev. 3.3","page":"295","title":"Intel® Virtualization Technology for Directed I/O Architecture Specification","type":"report","URL":"https://cdrdv2.intel.com/v1/dl/getContent/671081?explicitVersion=true"},
  {"id":"intelOpenCLSharedVirtual2014","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Intel"}],"citation-key":"intelOpenCLSharedVirtual2014","issued":{"date-parts":[["2014",9,11]]},"language":"en","title":"OpenCL™ 2.0 Shared Virtual Memory Overview","type":"webpage","URL":"https://www.intel.com/content/www/us/en/developer/articles/technical/opencl-20-shared-virtual-memory-overview.html"},
  {"id":"intelPCISIGSingleRoot2008","abstract":"As virtualized server deployment increases, virtualization technologies continue to evolve especially in the area of I/O performance. Within the industry, significant effort has been expended to increase the effectiveness of hardware resource utilization (i.e., application execution) through the use of virtualization technologies. The Single Root I/O Virtualization and Sharing Specification (SR-IOV) defines extensions to the PCI Express* (PCIe*) specification suite to enable multiple System Images (SI) or Virtual Machines (VMs/Guests) in the virtualized environment to share PCI hardware resources","author":[{"literal":"Intel"}],"citation-key":"intelPCISIGSingleRoot2008","issued":{"date-parts":[["2008"]]},"language":"en","number":"Rev. 06/08-001US","page":"4","source":"Zotero","title":"PCI-SIG Single Root I/O Virtualization (SR-IOV) Support in Intel® Virtualization Technology for Connectivity","type":"report","URL":"https://www.intel.com/content/dam/doc/white-paper/pci-sig-single-root-io-virtualization-support-in-virtualization-technology-for-connectivity-paper.pdf"},
  {"id":"intelUpdatesIntelNextGen2021","abstract":"Rich platform enhancements and built-in acceleration engines will address most demanding workloads from cloud to the edge.","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Intel"},{"family":"Spelman","given":"Lisa"}],"citation-key":"intelUpdatesIntelNextGen2021","container-title":"Intel","issued":{"date-parts":[["2021",6,29]]},"language":"en","title":"Updates on Intel’s Next-Gen Data Center Platform, Sapphire Rapids","type":"webpage","URL":"https://www.intel.com/content/www/us/en/newsroom/opinion/updates-next-gen-data-center-platform-sapphire-rapids.html"},
  {"id":"kressinSnapdragon855Deep","accessed":{"date-parts":[["2021",12,31]]},"author":[{"family":"Kressin","given":"Keith"}],"citation-key":"kressinSnapdragon855Deep","title":"Snapdragon 855 Deep Dives Intro","type":"speech","URL":"https://en.wikichip.org/w/images/e/e6/snapdragon-855-deep-dives-intro-keith-kressin.pdf"},
  {"id":"larabelMesa20Nouveau2020","abstract":"The open-source NVIDIA 'Nouveau' driver stack reached a new milestone today with the user-space code in Mesa 20.2 finally flipping on the Heterogeneous Memory Management (HMM) support.","accessed":{"date-parts":[["2021",12,31]]},"author":[{"family":"Larabel","given":"Michael"}],"citation-key":"larabelMesa20Nouveau2020","issued":{"date-parts":[["2020",7,14]]},"language":"en","title":"Mesa 20.2's Nouveau Enables HMM, OpenCL SVM Now Supported","type":"webpage","URL":"https://www.phoronix.com/scan.php?page=news_item&px=Mesa-20.2-Nouveau-HMM"},
  {"id":"larabelRadeonROCmReleased2021","abstract":"AMD has released ROCm 4.3 as the newest version of their Radeon Open eCosystem stack for providing open-source GPU compute and CUDA portability for their supported graphics processors under Linux","accessed":{"date-parts":[["2021",12,31]]},"author":[{"family":"Larabel","given":"Michael"}],"citation-key":"larabelRadeonROCmReleased2021","issued":{"date-parts":[["2021",8,3]]},"language":"en","title":"Radeon ROCm 4.3 Released With HMM Allocations, Many Other Improvements","type":"webpage","URL":"https://www.phoronix.com/scan.php?page=news_item&px=Radeon-ROCm-4.3"},
  {"id":"liEvaluatingModernGPU2020","abstract":"High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.","accessed":{"date-parts":[["2021",12,23]]},"author":[{"family":"Li","given":"Ang"},{"family":"Song","given":"Shuaiwen Leon"},{"family":"Chen","given":"Jieyang"},{"family":"Li","given":"Jiajia"},{"family":"Liu","given":"Xu"},{"family":"Tallent","given":"Nathan"},{"family":"Barker","given":"Kevin"}],"citation-key":"liEvaluatingModernGPU2020","container-title":"IEEE Transactions on Parallel and Distributed Systems","container-title-short":"IEEE Trans. Parallel Distrib. Syst.","DOI":"10/ggqsjd","ISSN":"1045-9219, 1558-2183, 2161-9883","issue":"1","issued":{"date-parts":[["2020",1,1]]},"page":"94-110","source":"arXiv.org","title":"Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect","title-short":"Evaluating Modern GPU Interconnect","type":"article-journal","URL":"http://arxiv.org/abs/1903.04611","volume":"31"},
  {"id":"linuxkernelAMDgpuDriver","accessed":{"date-parts":[["2021",12,27]]},"author":[{"literal":"Linux Kernel Dev Community"}],"citation-key":"linuxkernelAMDgpuDriver","title":"drm/amdgpu AMDgpu driver — The Linux Kernel documentation","type":"webpage","URL":"https://www.kernel.org/doc/html/v5.0/gpu/amdgpu.html"},
  {"id":"linuxkernelHeterogeneousMemoryManagement","accessed":{"date-parts":[["2021",12,31]]},"author":[{"family":"Linux Kernel Dev Community","given":""}],"citation-key":"linuxkernelHeterogeneousMemoryManagement","title":"Heterogeneous Memory Management (HMM) — The Linux Kernel documentation","type":"webpage","URL":"https://www.kernel.org/doc/html/v5.11/vm/hmm.html"},
  {"id":"linuxkernelLinuxAndTheDevicetree","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Linux Kernel Dev Community"},{"family":"Likely","given":"Grant"}],"citation-key":"linuxkernelLinuxAndTheDevicetree","title":"Linux and the Devicetree — The Linux Kernel documentation","type":"webpage","URL":"https://www.kernel.org/doc/html/latest/devicetree/usage-model.html"},
  {"id":"LVC20309ArmSMMU","abstract":"The Qualcomm Adreno GPU pushes the boundaries of the ARM SMMUv2 architecture in new and interesting ways. This presentation will discuss some of the new proposed GPU specific features for the ARM SMMUv2 driver such as split pagetables and pagetable switching and future enhancements to improve the GPU/SMMU relationship.","accessed":{"date-parts":[["2021",12,29]]},"citation-key":"LVC20309ArmSMMU","container-title":"Linaro","language":"en","title":"LVC20-309 The Arm SMMU and the Adreno GPU","type":"webpage","URL":"https://connect.linaro.org/resources/lvc20/lvc20-309/"},
  {"id":"markettosPositionPaperDefending2020","abstract":"We propose new solutions that can efficiently address the problem of malicious memory access from pluggable computer peripherals and microcontrollers embedded within a system-on-chip. This problem represents a serious emerging threat to total-system computer security. Previous work has shown that existing defenses are insufficient and poorly deployed, in part due to performance concerns. In this paper we explore the threat and its implications for system architecture. We propose a range of protection techniques, from lightweight to heavyweight, across different classes of systems. We consider how emerging capability architectures (and specifically the CHERI protection model) can enhance protection and provide a convenient bridge to describe interactions among software and hardware components. Finally, we describe how new schemes may be more efficient than existing defenses.","accessed":{"date-parts":[["2021",12,30]]},"author":[{"family":"Markettos","given":"A. Theodore"},{"family":"Baldwin","given":"John"},{"family":"Bukin","given":"Ruslan"},{"family":"Neumann","given":"Peter G."},{"family":"Moore","given":"Simon W."},{"family":"Watson","given":"Robert N. M."}],"citation-key":"markettosPositionPaperDefending2020","container-title":"Hardware and Architectural Support for Security and Privacy","DOI":"10/gnckmg","event-place":"Virtual Greece","event-title":"HASP '20: Hardware and Architectural Support for Security and Privacy","ISBN":"978-1-4503-8898-6","issued":{"date-parts":[["2020",10,17]]},"language":"en","page":"1-9","publisher":"ACM","publisher-place":"Virtual Greece","source":"DOI.org (Crossref)","title":"Position Paper:Defending Direct Memory Access with CHERI Capabilities","title-short":"Position Paper","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3458903.3458910"},
  {"id":"markettosThunderclapExploringVulnerabilities2019","abstract":"Direct Memory Access (DMA) attacks have been known for many years: DMA-enabled I/O peripherals have complete access to the state of a computer and can fully compromise it including reading and writing all of system memory. With the popularity of Thunderbolt 3 over USB Type-C and smart internal devices, opportunities for these attacks to be performed casually with only seconds of physical access to a computer have greatly broadened. In response, commodity hardware and operatingsystem (OS) vendors have incorporated support for Input-Ouptut Memory Management Units (IOMMUs), which impose memory protection on DMA, and are widely believed to protect against DMA attacks. We investigate the state-of-the-art in IOMMU protection across OSes using a novel I/O-security research platform, and ﬁnd that current protections fall short when faced with a functional network peripheral that uses its complex interactions with the OS for ill intent. We describe vulnerabilities in macOS, FreeBSD, and Linux, which notionally utilize IOMMUs to protect against DMA attackers. Windows uses the IOMMU only in limited cases. and it remains vulnerable. Using Thunderclap, an open-source FPGA research platform that we built, we explore new classes of OS vulnerability arising from inadequate use of the IOMMU. The complex vulnerability space for IOMMUexposed shared memory available to DMA-enabled peripherals allows attackers to extract private data (snifﬁng cleartext VPN trafﬁc) and hijack kernel control ﬂow (launching a root shell) in seconds using devices such as USB-C projectors or power adapters. We have worked closely with OS vendors to remedy these vulnerability classes, and they have now shipped substantial feature improvements and mitigations as a result of our work.","accessed":{"date-parts":[["2021",12,31]]},"author":[{"family":"Markettos","given":"A. Theodore"},{"family":"Rothwell","given":"Colin"},{"family":"Gutstein","given":"Brett F."},{"family":"Pearce","given":"Allison"},{"family":"Neumann","given":"Peter G."},{"family":"Moore","given":"Simon W."},{"family":"Watson","given":"Robert N. M."}],"citation-key":"markettosThunderclapExploringVulnerabilities2019","container-title":"Proceedings 2019 Network and Distributed System Security Symposium","DOI":"10/gjh62d","event-place":"San Diego, CA","event-title":"Network and Distributed System Security Symposium","ISBN":"978-1-891562-55-6","issued":{"date-parts":[["2019"]]},"language":"en","publisher":"Internet Society","publisher-place":"San Diego, CA","source":"DOI.org (Crossref)","title":"Thunderclap: Exploring Vulnerabilities in Operating System IOMMU Protection via DMA from Untrustworthy Peripherals","title-short":"Thunderclap","type":"paper-conference","URL":"https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_05A-1_Markettos_paper.pdf"},
  {"id":"markuzeTrueIOMMUProtection2016","abstract":"Malicious I/O devices might compromise the OS using DMAs. The OS therefore utilizes the IOMMU to map and unmap every target buffer right before and after its DMA is processed, thereby restricting DMAs to their designated locations. This usage model, however, is not truly secure for two reasons: (1) it provides protection at page granularity only, whereas DMA buffers can reside on the same page as other data; and (2) it delays DMA buffer unmaps due to performance considerations, creating a vulnerability window in which devices can access in-use memory. We propose that OSes utilize the IOMMU differently, in a manner that eliminates these two flaws. Our new usage model restricts device access to a set of shadow DMA buffers that are never unmapped, and it copies DMAed data to/from these buffers, thus providing sub-page protection while eliminating the aforementioned vulnerability window. Our key insight is that the cost of interacting with, and synchronizing access to the slow IOMMU hardware---required for zero-copy protection against devices---make copying preferable to zero-copying. We implement our model in Linux and evaluate it with standard networking benchmarks utilizing a 40,Gb/s NIC. We demonstrate that despite being more secure than the safest preexisting usage model, our approach provides up to 5x higher throughput. Additionally, whereas it is inherently less scalable than an IOMMU-less (unprotected) system, our approach incurs only 0%--25% performance degradation in comparison.","accessed":{"date-parts":[["2023",10,2]]},"author":[{"family":"Markuze","given":"Alex"},{"family":"Morrison","given":"Adam"},{"family":"Tsafrir","given":"Dan"}],"citation-key":"markuzeTrueIOMMUProtection2016","collection-title":"ASPLOS '16","container-title":"Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems","DOI":"10.1145/2872362.2872379","event-place":"New York, NY, USA","ISBN":"978-1-4503-4091-5","issued":{"date-parts":[["2016",3,25]]},"page":"249–262","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"True IOMMU Protection from DMA Attacks: When Copy is Faster than Zero Copy","title-short":"True IOMMU Protection from DMA Attacks","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/2872362.2872379"},
  {"id":"minEMOGIEfficientMemoryaccess2021","abstract":"Modern analytics and recommendation systems are increasingly based on graph data that capture the relations between entities being analyzed. Practical graphs come in huge sizes, offer massive parallelism, and are stored in sparse-matrix formats such as compressed sparse row (CSR). To exploit the massive parallelism, developers are increasingly interested in using GPUs for graph traversal. However, due to their sizes, graphs often do not fit into the GPU memory. Prior works have either used input data preprocessing/partitioning or unified virtual memory (UVM) to migrate chunks of data from the host memory to the GPU memory. However, the large, multi-dimensional, and sparse nature of graph data presents a major challenge to these schemes and results in significant amplification of data movement and reduced effective data throughput. In this work, we propose EMOGI, an alternative approach to traverse graphs that do not fit in GPU memory using direct cache-line-sized access to data stored in host memory.","accessed":{"date-parts":[["2021",11,8]]},"author":[{"family":"Min","given":"Seung Won"},{"family":"Mailthody","given":"Vikram Sharma"},{"family":"Qureshi","given":"Zaid"},{"family":"Xiong","given":"Jinjun"},{"family":"Ebrahimi","given":"Eiman"},{"family":"Hwu","given":"Wen-mei"}],"citation-key":"minEMOGIEfficientMemoryaccess2021","container-title":"arXiv:2006.06890 [cs]","issued":{"date-parts":[["2021",1,14]]},"language":"en","source":"arXiv.org","title":"EMOGI: Efficient Memory-access for Out-of-memory Graph-traversal In GPUs","title-short":"EMOGI","type":"article-journal","URL":"http://arxiv.org/abs/2006.06890"},
  {"id":"morganIOMMUProtectionAttacks2018","abstract":"Input/output (I/O) attacks have received increasing attention during the last decade. These attacks are performed by malicious peripherals that make read or write accesses to DRAM memory or to memory embedded in other peripherals, through DMA (Direct Memory Access) requests. Some protection mechanisms have been implemented in modern architectures to face these attacks. A typical example is the IOMMU (Input-Output Memory Management Unit). However, such mechanisms may not be properly configured and used by the firmware and the operating system. This paper describes a design weakness that we discovered in the configuration of an IOMMU and a possible exploitation scenario that would allow a malicious peripheral to bypass the underlying protection mechanism. The exploitation scenario is implemented for Intel architectures, with a PCI Express peripheral Field Programmable Gate Array, based on Intel specifications and Linux source code analysis. Finally, as a proof of concept, a Linux rootkit based on the attack presented in this paper is implemented.","accessed":{"date-parts":[["2021",12,29]]},"author":[{"family":"Morgan","given":"Benoît"},{"family":"Alata","given":"Éric"},{"family":"Nicomette","given":"Vincent"},{"family":"Kaâniche","given":"Mohamed"}],"citation-key":"morganIOMMUProtectionAttacks2018","container-title":"Journal of the Brazilian Computer Society","container-title-short":"Journal of the Brazilian Computer Society","DOI":"10/gf8xn6","ISSN":"1678-4804","issue":"1","issued":{"date-parts":[["2018",1,9]]},"page":"2","source":"BioMed Central","title":"IOMMU protection against I/O attacks: a vulnerability and a proof of concept","title-short":"IOMMU protection against I/O attacks","type":"article-journal","URL":"https://doi.org/10.1186/s13173-017-0066-7","volume":"24"},
  {"id":"nvidiacorporationCUDAProgrammingGuide","abstract":"The programming guide to the CUDA model and interface.","accessed":{"date-parts":[["2021",11,11]]},"archive_location":"Programming Guides","author":[{"literal":"NVIDIA Corporation"}],"citation-key":"nvidiacorporationCUDAProgrammingGuide","language":"en-us","title":"CUDA C++ Programming Guide v11.5.0","type":"document","URL":"https://docs.nvidia.com/cuda/archive/11.5.0/cuda-c-programming-guide/index.html"},
  {"id":"nvidiacorporationNVIDIAJetsonTX22017","abstract":"Today at an AI meetup in San Francisco, NVIDIA launched Jetson TX2 and the JetPack 3.0 AI SDK. Jetson is the world’s leading low-power embedded platform, enabling server-class AI compute performance…","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"NVIDIA Corporation"}],"citation-key":"nvidiacorporationNVIDIAJetsonTX22017","container-title":"NVIDIA Developer Blog","issued":{"date-parts":[["2017",3,8]]},"language":"en-US","title":"NVIDIA Jetson TX2 Delivers Twice the Intelligence to the Edge","type":"webpage","URL":"https://developer.nvidia.com/blog/jetson-tx2-delivers-twice-intelligence-edge/"},
  {"id":"nvidiacorporationNVIDIAMultiInstanceGPU2020","accessed":{"date-parts":[["2021",12,27]]},"author":[{"literal":"NVIDIA Corporation"}],"citation-key":"nvidiacorporationNVIDIAMultiInstanceGPU2020","issued":{"date-parts":[["2020",11]]},"number":"TB-10226-001_v01","title":"NVIDIA Multi-Instance GPU and NVIDIA  Virtual Compute Server - Technical Brief","type":"report","URL":"https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/solutions/resources/documents1/TB-10226-001_v01.pdf"},
  {"id":"nvidiacorporationPascalMMUFormat2016","accessed":{"date-parts":[["2021",12,27]]},"author":[{"literal":"NVIDIA Corporation"}],"citation-key":"nvidiacorporationPascalMMUFormat2016","issued":{"date-parts":[["2016",11,22]]},"title":"Pascal MMU Format Changes - OpenGPUDoc","type":"webpage","URL":"https://nvidia.github.io/open-gpu-doc/pascal/gp100-mmu-format.pdf"},
  {"id":"nvidiacorporationS8868CUDAXavier2018","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"NVIDIA Corporation"},{"family":"Bhat","given":"Anshuman"},{"family":"Dasadhikari","given":"Saikat"}],"citation-key":"nvidiacorporationS8868CUDAXavier2018","event-place":"GTC 2018","issued":{"date-parts":[["2018",3,29]]},"publisher-place":"GTC 2018","title":"S8868 - CUDA on Xavier","type":"speech","URL":"https://on-demand.gputechconf.com/gtc/2018/presentation/s8868-cuda-on-xavier-what-is-new.pdf"},
  {"id":"nvidiacorporationUnifiedMemoryCUDA2017","abstract":"This post introduces CUDA programming with Unified Memory, a single memory address space that is accessible from any GPU or CPU in a system.","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"NVIDIA Corporation"},{"family":"Harris","given":"Mark"}],"citation-key":"nvidiacorporationUnifiedMemoryCUDA2017","container-title":"NVIDIA Developer Blog","issued":{"date-parts":[["2017",6,20]]},"language":"en-US","title":"Unified Memory for CUDA Beginners","type":"webpage","URL":"https://developer.nvidia.com/blog/unified-memory-cuda-beginners/"},
  {"id":"PCIeATSR1.1","author":[{"literal":"PCI-SIG"}],"citation-key":"PCIeATSR1.1","issued":{"date-parts":[["2009",1,26]]},"language":"en","number":"Revision 1.1","page":"54","source":"Zotero","title":"Address Translation Services","type":"report","URL":"https://composter.com.ua/documents/ats_r1.1_26Jan09.pdf"},
  {"id":"qualcommSnapdragonGenMobile2021","abstract":"Our most advanced 5G platform ever with ground-breaking innovations in AI, photography, gaming, and connectivity.","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Qualcomm"}],"citation-key":"qualcommSnapdragonGenMobile2021","issued":{"date-parts":[["2021",11,17]]},"language":"en","title":"Snapdragon 8 Gen 1 Mobile Platform","type":"webpage","URL":"https://www.qualcomm.com/products/snapdragon-8-gen-1-mobile-platform"},
  {"id":"samsungExynos980Release","abstract":"With multi-mode capabilities and sub-6GHz speed at up to 2.55Gbps, the Exynos 980 will help bring 5G connectivity to more mobile devices","accessed":{"date-parts":[["2021",12,31]]},"author":[{"literal":"Samsung Semiconductor"}],"citation-key":"samsungExynos980Release","language":"en","title":"Samsung Introduces its First 5G-Integrated Mobile Processor, the Exynos 980","type":"webpage","URL":"https://www.samsung.com/semiconductor/minisite/exynos/newsroom/pressrelease/samsung-introduces-its-first-5g-integrated-mobile-processor-the-exynos-980/"},
  {"id":"suzukiGPUvmWhyNot2014","accessed":{"date-parts":[["2021",12,27]]},"author":[{"family":"Suzuki","given":"Yusuke"},{"family":"Kato","given":"Shinpei"},{"family":"Yamada","given":"Hiroshi"},{"family":"Kono","given":"Kenji"}],"citation-key":"suzukiGPUvmWhyNot2014","event-title":"2014 USENIX Annual Technical Conference (USENIX ATC 14)","ISBN":"978-1-931971-10-2","issued":{"date-parts":[["2014"]]},"language":"en","page":"109-120","source":"www.usenix.org","title":"GPUvm: Why Not Virtualizing GPUs at the Hypervisor?","title-short":"GPUvm","type":"paper-conference","URL":"https://www.usenix.org/conference/atc14/technical-sessions/presentation/suzuki"},
  {"id":"tineVortexExtendingRISCV2021","abstract":"The importance of open-source hardware and software has been increasing. However, despite GPUs being one of the more popular accelerators across various applications, there is very little opensource GPU infrastructure in the public domain. We argue that one of the reasons for the lack of open-source infrastructure for GPUs is rooted in the complexity of their ISA and software stacks. In this work, we first propose an ISA extension to RISC-V that supports GPGPUs and graphics. The main goal of the ISA extension proposal is to minimize the ISA changes so that the corresponding changes to the open-source ecosystem are also minimal, which makes for a sustainable development ecosystem. To demonstrate the feasibility of the minimally extended RISC-V ISA, we implemented the complete software and hardware stacks of Vortex on FPGA. Vortex is a PCIe-based soft GPU that supports OpenCL and OpenGL. Vortex can be used in a variety of applications, including machine learning, graph analytics, and graphics rendering. Vortex can scale up to 32 cores on an Altera Stratix 10 FPGA, delivering a peak performance of 25.6 GFlops at 200 Mhz.","accessed":{"date-parts":[["2021",11,22]]},"author":[{"family":"Tine","given":"Blaise"},{"family":"Yalamarthy","given":"Krishna Praveen"},{"family":"Elsabbagh","given":"Fares"},{"family":"Hyesoon","given":"Kim"}],"citation-key":"tineVortexExtendingRISCV2021","container-title":"MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture","DOI":"10/gnj4cn","event-place":"Virtual Event Greece","event-title":"MICRO '21: 54th Annual IEEE/ACM International Symposium on Microarchitecture","ISBN":"978-1-4503-8557-2","issued":{"date-parts":[["2021",10,18]]},"language":"en","page":"754-766","publisher":"ACM","publisher-place":"Virtual Event Greece","source":"DOI.org (Crossref)","title":"Vortex: Extending the RISC-V ISA for GPGPU and 3D-Graphics","title-short":"Vortex","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3466752.3480128"},
  {"id":"UnderstandingIommuLinux","accessed":{"date-parts":[["2021",12,27]]},"citation-key":"UnderstandingIommuLinux","title":"Understanding the iommu Linux grub File Configuration","type":"webpage","URL":"https://community.mellanox.com/s/article/understanding-the-iommu-linux-grub-file-configuration"},
  {"id":"wangGrusUnifiedmemoryefficientHighperformance2021","abstract":"Today’s GPU graph processing frameworks face scalability and efficiency issues as the graph size exceeds GPU-dedicated memory limit. Although recent GPUs can over-subscribe memory with Unified Memory (UM), they incur significant overhead when handling graph-structured data. In addition, many popular processing frameworks suffer sub-optimal efficiency due to heavy atomic operations when tracking the active vertices. This article presents Grus, a novel system framework that allows GPU graph processing to stay competitive with the ever-growing graph complexity. Grus improves space efficiency through a UM trimming scheme tailored to the data access behaviors of graph workloads. It also uses a lightweight frontier structure to further reduce atomic operations. With easy-to-use interface that abstracts the above details, Grus shows up to 6.4× average speedup over the state-of-the-art in-memory GPU graph processing framework. It allows one to process large graphs of 5.5 billion edges in seconds with a single GPU.","accessed":{"date-parts":[["2021",12,5]]},"author":[{"family":"Wang","given":"Pengyu"},{"family":"Wang","given":"Jing"},{"family":"Li","given":"Chao"},{"family":"Wang","given":"Jianzong"},{"family":"Zhu","given":"Haojin"},{"family":"Guo","given":"Minyi"}],"citation-key":"wangGrusUnifiedmemoryefficientHighperformance2021","container-title":"ACM Transactions on Architecture and Code Optimization","container-title-short":"ACM Trans. Archit. Code Optim.","DOI":"10/gnpp58","ISSN":"1544-3566","issue":"2","issued":{"date-parts":[["2021",2,9]]},"page":"22:1–22:25","source":"March 2021","title":"Grus: Toward Unified-memory-efficient High-performance Graph Processing on GPU","title-short":"Grus","type":"article-journal","URL":"https://doi.org/10.1145/3444844","volume":"18"},
  {"id":"watsonCHERIHybridCapabilitySystem2015","abstract":"CHERI extends a conventional RISC InstructionSet Architecture, compiler, and operating system to support ﬁne-grained, capability-based memory protection to mitigate memory-related vulnerabilities in C-language TCBs. We describe how CHERI capabilities can also underpin a hardware-software object-capability model for application compartmentalization that can mitigate broader classes of attack. Prototyped as an extension to the open-source 64-bit BERI RISC FPGA softcore processor, FreeBSD operating system, and LLVM compiler, we demonstrate multiple orders-of-magnitude improvement in scalability, simpliﬁed programmability, and resulting tangible security beneﬁts as compared to compartmentalization based on pure Memory-Management Unit (MMU) designs. We evaluate incrementally deployable CHERI-based compartmentalization using several real-world UNIX libraries and applications.","accessed":{"date-parts":[["2021",12,30]]},"author":[{"family":"Watson","given":"Robert N.M."},{"family":"Woodruff","given":"Jonathan"},{"family":"Neumann","given":"Peter G."},{"family":"Moore","given":"Simon W."},{"family":"Anderson","given":"Jonathan"},{"family":"Chisnall","given":"David"},{"family":"Dave","given":"Nirav"},{"family":"Davis","given":"Brooks"},{"family":"Gudka","given":"Khilan"},{"family":"Laurie","given":"Ben"},{"family":"Murdoch","given":"Steven J."},{"family":"Norton","given":"Robert"},{"family":"Roe","given":"Michael"},{"family":"Son","given":"Stacey"},{"family":"Vadera","given":"Munraj"}],"citation-key":"watsonCHERIHybridCapabilitySystem2015","container-title":"2015 IEEE Symposium on Security and Privacy","DOI":"10/gfpgzz","event-place":"San Jose, CA","event-title":"2015 IEEE Symposium on Security and Privacy (SP)","ISBN":"978-1-4673-6949-7","issued":{"date-parts":[["2015",5]]},"language":"en","page":"20-37","publisher":"IEEE","publisher-place":"San Jose, CA","source":"DOI.org (Crossref)","title":"CHERI: A Hybrid Capability-System Architecture for Scalable Software Compartmentalization","title-short":"CHERI","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/7163016/"},
  {"id":"xiangExploitingUniformVector2013","abstract":"State-of-art graphics processing units (GPUs) employ the single-instruction multiple-data (SIMD) style execution to achieve both high computational throughput and energy efficiency. As previous works have shown, there exists significant computational redundancy in SIMD execution, where different execution lanes operate on the same operand values. Such value locality is referred to as uniform vectors. In this paper, we first show that besides redundancy within a uniform vector, different vectors can also have the identical values. Then, we propose detailed architecture designs to exploit both types of redundancy. For redundancy within a uniform vector, we propose to either extend the vector register file with token bits or add a separate small scalar register file to eliminate redundant computations as well as redundant data storage. For redundancy across different uniform vectors, we adopt instruction reuse, proposed originally for CPU architectures, to detect and eliminate redundancy. The elimination of redundant computations and data storage leads to both significant energy savings and performance improvement. Furthermore, we propose to leverage such redundancy to protect arithmetic-logic units (ALUs) and register files against hardware errors. Our detailed evaluation shows that our proposed design has low hardware overhead and achieves performance gains, up to 23.9% and 12.0% on average, along with energy savings, up to 24.8% and 12.6% on average, as well as a 21.1% and 14.1% protection coverage for ALUs and register files, respectively.","accessed":{"date-parts":[["2021",11,22]]},"author":[{"family":"Xiang","given":"Ping"},{"family":"Yang","given":"Yi"},{"family":"Mantor","given":"Mike"},{"family":"Rubin","given":"Norm"},{"family":"Hsu","given":"Lisa R."},{"family":"Zhou","given":"Huiyang"}],"citation-key":"xiangExploitingUniformVector2013","collection-title":"ICS '13","container-title":"Proceedings of the 27th international ACM conference on International conference on supercomputing","DOI":"10/gnj4ds","event-place":"New York, NY, USA","ISBN":"978-1-4503-2130-3","issued":{"date-parts":[["2013",6,10]]},"page":"433–442","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Exploiting uniform vector instructions for GPGPU performance, energy efficiency, and opportunistic reliability enhancement","type":"paper-conference","URL":"https://doi.org/10.1145/2464996.2465022"},
  {"id":"yuArchitectureSupportedRegister2016","abstract":"GPGPU provides abundant hardware resources to support a large number of light-weighted threads. They are organized into blocks and run in warps. All threads of a block must be dispatched to one stream multiprocessor (SM) of GPGPU together. When the remaining resources of an SM cannot support one more block, all threads of the block are held back until former blocks retire from the SM. We found that the register file is prone to be the most limited one among all the resources, especially for SMs with less registers. Meanwhile, we revealed the dynamics of a thread's register requirement: only part of its pre-allocated registers are used for different instructions at run time. This results in considerable register underutilization.We proposed the architecture supported register stash (ASRS). It removes the limitation of registers when dispatching blocks. The hardware registers are allocated at run time according to each instruction's live registers, which can be analyzed statically by a compiler. When the hardware registers cannot meet the requirements of all running warps, some warps are suspended and their registers are reclaimed temporarily. The data in these registers are stashed to memory. On the other hand, if there are spare hardware registers, it will start a new warp or resume a suspended warp after all the warp's stashed register data are loaded from memory. The intra-block synchronization is also taken care of when some of the warps of the same block are not schedulable due to the ASRS.The ASRS alleviates the register underutilization and improves performance without modifying the current programming model or demanding extra effort from the programmers. It also enables an SM with limited registers that cannot even support a single block to execute it. Besides, it helps lower the register file energy consumption and increase the power efficiency. The ASRS achieved speedups of 1.59 and 1.14 when the registers of each SM are limited to 8K and 16K respectively with an insignificant overhead. The speedups compared with the infinite register files are 0.84 and 0.98 with 8K and 16K registers respectively. Compared with the baseline 32K register file, the ASRS decreases the 8K and 16K register file energy consumption to 66.5% and 75.8% respectively. Their power efficiencies (in ratio of performance and power) are increased to 1.29x and 1.31x respectively. Register requirement of GPGPU varies among different kernels and during run time.Register file (RF) capacity limits the schedulable warps and performance.Reducing RF capacity lowers energy consumption and area.We proposed a method to support more warps with limited registers.It gains significant speedup and a higher energy efficiency with a smaller RF.","accessed":{"date-parts":[["2021",11,22]]},"author":[{"family":"Yu","given":"Licheng"},{"family":"Pei","given":"Yulong"},{"family":"Chen","given":"Tianzhou"},{"family":"Wu","given":"Minghui"}],"citation-key":"yuArchitectureSupportedRegister2016","container-title":"Journal of Parallel and Distributed Computing","container-title-short":"J. Parallel Distrib. Comput.","DOI":"10/f8cwmn","ISSN":"0743-7315","issue":"C","issued":{"date-parts":[["2016",3,1]]},"page":"25–36","source":"March 2016","title":"Architecture supported register stash for GPGPU","type":"article-journal","URL":"https://doi.org/10.1016/j.jpdc.2015.12.003","volume":"89"}
]
