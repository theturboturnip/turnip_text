[section("Introduction/Extended Abstract")]{
    # 500-1000 words

    # It should be comprehensible to an intelligent but non-expert audience 
    # (Cambridge Trust Panels, for example) and so explain acronyms or leave them 
    # out. 

    # Talk about your ideaâ€™s wider application and why it is an important, exciting 
    # topic to research.


    CHERI[footnote]{[url("https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/")]} is a project within the Cambridge Computer Lab which adds fine-grained memory security to computer architecture.
    Instead of allowing programs to access arbitrary memory locations, which malicious code can exploit to access data it shouldn't, CHERI requires that memory is accessed through unforgeable `capabilities'.
    The computer hardware enforces that if a program receives a capability referring to a specific area of memory, the program cannot manipulate the capability to point outside that range.
    When a capability is `dereferenced', e.g. the memory it points to is accessed, one can be sure that it points to valid memory the program is allowed to access.
    This feature can resolve many security issues and bugs in modern programs, and industry seems to agree that CHERI is worthwhile. 
    Arm Ltd have developed a test chip using these capabilities in collaboration with Cambridge, and CHERI overall is aiming for wide adoption.

    CHERI initially focused on Central Processing Units (CPUs), the core elements of modern computers, and the CAPcelerate[footnote]{[url("https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/V000381/1")]} sub-project (also at Cambridge) is now investigating capabilities for hardware accelerators such as Graphics Processing Units (GPUs).
    While a modern computer may have eight to sixteen CPUs, which are flexible, GPUs have thousands of small less-flexible cores.
    While GPUs were initially used for graphics, their high degree of parallelism is also useful for large-scale data processing such as Machine Learning (ML) in the cloud.
    GPUs can hold data that's sensitive at small-scale (e.g. data for displaying personalized web pages) and at large-scale (e.g. audio data used for training speech recognition), and adding capabilities to GPUs help protect this data from attackers.
    However, as the next section will show, CHERI's CPU-centric capabilities may need to adapt to work well with GPUs.

    #----------------------
    [subsection("Problem Definition", num=False)]{
        Current systems that use CHERI capabilities make a few basic assumptions:

        [enumerate]{
            [item]{
                Capabilities hold memory addresses that are controlled by the Operating System (OS) from the CPU
            }
            [item]{
                Capabilities have the same format
            }
            [item]{
                Capabilities can be used in the same way from anywhere
            }
        }

        This is reasonable for homogeneous multi-CPU systems, where memory is used in the same way by all elements, or systems where main memory is shared between the CPUs and GPUs.
        However, high-performance external GPUs tend to use memory differently than CPUs, and could benefit from handling capabilities differently.
        This would create two distinct `capability domains': that of the CPU and of the GPU.
        My proposal is to investigate communication between different capability domains specifically in the context of differing CPU and GPU capabilities.
        In the rest of this section I'll outline some of the CPU-GPU differences.

        External GPUs typically have their own memory that can only be accessed by the CPU through PCIe (a relatively slow connection).
        Instead of going through the CPU and operating system (OS), it's preferred to use direct GPU-to-GPU (e.g. NVLink) and GPU-to-device (GPUDirect) connections to transfer data.
        For example, a cloud server may have a direct connection between a GPU and an NVMe storage device for transferring large ML datasets.
        This principle is also extending to consumer devices with Microsoft DirectStorage, which reduces CPU overhead without entirely removing it, but this is much slower than the direct connections e.g. <10GB/s vs 100+GB/s.
        Overall, it's clear the GPU can benefit from not relying on the OS to do e.g. memory allocation.

        Unlike CPU programs, which can run for a long time, GPU tasks are often ephemeral.
        Rendering (generating an image of a 3D scene) is split into many short-lived tasks, and ML inference (evaluating a model to make decisions about data) typically doesn't take very long.
        This means the capabilities they hold only need to be valid for short amounts of time, and in some cases data itself can be short-lived.
        This could inform a new approach to capability [emph]{revocation}, a complex problem on the CPU[footnote]{[url("https://www.cl.cam.ac.uk/research/security/ctsrd/pdfs/2020oakland-cornucopia.pdf")]}, where capabilities could have a fixed lifetime instead of remaining valid forever.
        Graphics tasks in particular expose simple units of lifetime, such as the number of frames rendered onscreen - a capability could only be valid for a fixed number of frames.
        To exploit this for revocation, GPUs would need a unique representation of capabilities that may not be compatible with existing CPU representations.

        GPUs support special memory transactions beyond simple reads and writes, such as [enquote]{texture reads} for graphics workloads.
        These are more complex than memory accesses on the CPU, requiring dedicated hardware for performing filtering and decoding/decompressing image formats.
        On top of this, the GPU is often allowed to choose the format of images based on the situation, rather than the programmer controlling it like on the CPU.
        Because the GPU controls the image format, and has hardware to decode the format, allowing programs to read arbitrary bytes out of that data is unnecessary here.
        Separating [enquote]{texture capabilities} from normal capabilities could enforce that these are accessed through correct hardware, but as this hardware isn't available on CPUs one would have to prevent CPUs from dereferencing them.
    }

    #----------------------
    [subsection("Potential Approach", num=False)]{
        As shown above, GPU and CPU memory have different characteristics, and GPUs could benefit from having separate capability types.
        However, even if these capability types could only be dereferenced on the GPU, they will need to be created or manipulated from the CPU - the gap between domains needs to be crossed.
        I believe graphics-specific GPU drivers are a great place to start investigating this.

        Rather than handing out addresses in GPU memory, graphics APIs usually deal in `resource IDs'.
        The translation between resource IDs and GPU addresses is entirely delegated to the GPU driver, making it a natural starting point for adding domain-crossing.
        As CAPcelerate is investigating, naive implementations of this model can be insecure, but this does not affect the principles of domain-crossing.
        Once domain-crossing approaches have been developed in this context, they can be extended to support lower-level APIs.
    }
}

#-------------------
[section("Related Work")]{
    While capabilities themselves haven't yet addressed domain-crossing, there is a vast amount of previous work for sharing memory with legacy pointers.
    This section will highlight three broad topics that require domain crossing, which domain-crossing capability systems may need to be compatible with or build on.

    # ------------------------------------
    [subsection("DMA")]{
        Modern System on Chips (SoCs) can contain much more than just CPUs and GPUs.
        Mobile SoCs are a great example due to their high connectivity - custom hardware is included to support connections over mobile networks (4G/5G), Bluetooth, and Wi-Fi.
        On top of these, extra accelerators can be included e.g. for AI processing \
        [cite("qualcommSnapdragonGenMobile2021","kressinSnapdragon855Deep","samsungExynos980Release")].
        Internally, these peripherals will often contain their own CPUs, potentially with vastly different parameters to the main CPUs on the SoC.
        For example, Arm's Cortex R8 ([cite("armltdARMCortexR8Processor2016","armltdCortexR8")]) is a 32-bit processor for 5G modems, as well as hard-disk/SSD controllers, which has to interface with 64-bit systems (potentially with more than 4GB of addressable memory).

        These peripherals are usually configured using special registers which are exposed to the CPU as addresses in the memory map.
        The mapping of register to physical address is done statically at design time.
        Software must either be recompiled for each SoC with different mappings, or use some dynamic runtime solution to understand which peripherals are mapped to which addresses.
        Linux does the latter by parsing a SoC-specific device tree file at boot time[cite("linuxkernelLinuxAndTheDevicetree")].
        Peripherals will store larger datasets, such as decoded Bluetooth audio signals, in their own internal buffers (essentially a separate [emph]{domain} to the rest of the system).
        To access this data, the main CPU has to request a Direct Memory Access (DMA) from this buffer into main memory.

        DMAs are performed by setting registers in a DMA Controller (DMAC) peripheral[footnote]{One such example is the Arm AMBA DMA-330 [cite("armltdAMBADMAController2009")]}, including the base address in the source and destination domains.
        For a peripheral-to-memory transfer, the source address will be in the peripheral domain, and the destination address could be a physical address.
        The DMAC then pulls data from the peripheral domain and writes it out to the physical main memory domain, pulling data across the domain boundary.
        This is a very simple example of domain crossing for physically separated hardware domains, but it isn't enough for dynamic software domains.
        For example, accessing peripherals from Virtual Machines (VMs) adds a layer of translation between the guest-domains and the host-domains.
    }

    # ------------------------------------
    [subsection("Virtualization")]{
        When a VM tries to DMA data to/from a peripheral, it will specify a guest-domain address (guest-physical, or potentially guest-virtual) which needs to be translated into a host-physical address which the DMAC can use.
        AMD[cite("amdAMDIOVirtualizationTechnology2021")], Arm[cite("armltdArmSMMUVer3")], and Intel[cite("intelIOMMUSpecRev3p3")] each have their own IOMMU (I/O Memory Management Unit) design which performs this translation dynamically.
        Each device is assigned a page table, and devices can optionally be split into different domains with different tables.
        The IOMMU can reuse page tables from the MMU, effectively granting peripherals the same permissions and address space as a specific process or VM.

        Unfortunately, IOMMUs are only as secure as their configuration[footnote]{[cite("morganIOMMUProtectionAttacks2018","markuzeTrueIOMMUProtection2016")] are examples of IOMMU attacks relying on windows of misconfigured protection} can have high overheads.
        An I/O TLB is essential to avoid walking the page table on every DMA operation, but it may not be large enough to handle high VM counts.
        PCIe's Address Translation Service (ATS) takes a different approach, effectively distributing the IOTLB across PCIe devices[cite("pcisigAddressTranslationServices2009")].
        The PCIe device consults the ATS for guest-to-host address translations, and stores them in an internal cache.
        When it sends a DMA request, it applies a cached translation.
        # itself
        # already-translated host-domain physical address.
        The downside of this approach is that the system must trust the translated addresses the PCIe device spits back out, and trusting external peripherals is not always a good idea[cite("markettosThunderclapExploringVulnerabilities2019")].
        [cite("markettosPositionPaperDefending2020")] proposes CHERI-based alternatives to better protect the system from peripherals.
        # Of course, all of this work only protects main memory from peripherals, and not peripherals from the rest of the system.
        Peripherals that include their own external memory, like GPUs or network accelerators, still need to protect themselves from the rest of the system.

        # External GPUs that are multiplexed across multiple VMs, e.g. used by multiple VMs concurrently, can protect those VMs from each other by implementing PCIe Single-Root I/O Virtualization (SR-IOV).
        External GPUs that are used by multiple VMs concurrently can protect those VMs from each other by implementing PCIe Single-Root I/O Virtualization (SR-IOV)[cite("intelPCISIGSingleRoot2008")].
        The GPU exposes a single Physical Function (PF) to the hypervisor, which controls the whole device.
        Each guest VM is then given a different Virtual Function (VF) connected to the original PF, controlling a subset of the device.
        This system allows data transfer between the VF and the VM to avoid passing through the hypervisor stack, which is faster, but the GPUs still have to ensure the VFs they expose are correctly isolated.

        Modern GPUs use physical hardware segmentation and software support to effectively partition VMs from each other.
        AMD's MxGPU virtualization physically segments GPU memory and gives each VM a timeslice during which only their partition is active[cite("amdAMDApproachGPU2017")].
        When using SR-IOV, Nvidia's vGPU technology physically segments both the memory and the actual computation units, to avoid side-channel attacks, and allows multiple VMs to execute on different partitions at once.
        Integrated GPUs (e.g. Arm Mali GPUs, packaged on the same SoC as the CPU) don't necessarily use SR-IOV, but do isolate resources similarly[cite("armltdArmMaliG78AEProduct2020")].
    }

    # ------------------------------------
    [subsection("GPU Virtual Memory")]{
        On top of isolating VMs from each other, a further level of translation is added on the GPU side: Nvidia and AMD external GPUs have their own dedicated MMUs for virtual-to-physical address translation.
        This is in large part due to the idea of [enquote]{Shared Virtual Memory} (SVM), where the CPU and GPU share an address space, and pointers to SVM can be seamlessly dereferenced on both.
        OpenCL~2.0 and Nvidia's CUDA APIs both support this concept [cite("intelOpenCLSharedVirtual2014","nvidiacorporationUnifiedMemoryCUDA2017")] to varying degrees.

        In most cases, SVM has to be allocated through special API functions (e.g. CUDA Managed Memory, OpenCL Buffer Memory) and coherence is coarse: on CUDA, SVM pages are dynamically migrated between the CPU and GPU on-demand, rather than synchronizing individual loads and stores.
        This means page tables on CUDA devices track the [enquote]{Aperture} of each page, denoting which memory the page currently occupies[cite("nvidiacorporationPascalMMUFormat2016")].
        Not content with this, Nvidia and AMD are pushing for more granular coherence and increasing memory sharing.

        In the HPC space, some systems connect the CPU and GPU using Nvidia's proprietary NVLINK connection standard, rather than PCIe [cite("ibmpower9nputeamFunctionalityPerformanceNVLink2018")].
        These connections have much higher bandwidth, and allow for coherence at the cache-line granularity on SVM [cite(("nvidiacorporationCUDAProgrammingGuide", "N.1.7"))] and directly accessing CPU page tables from the GPU [cite(("nvidiacorporationCUDAProgrammingGuide", "N.1.6"))].
        The latter is only available on IBM Power9 systems, but expands SVM to [emph]{all memory in the system}, not just memory specifically allocated by CUDA.
        On more conventional PCIe based systems, this isn't yet possible, and drivers have to manually synchronize the GPU and CPU MMUs.
        Linux has introduced a Heterogeneous Memory Management kernel module[cite("linuxkernelHeterogeneousMemoryManagement","hubbardGPUsHMMHeterogeneous2017")] to make this process easier, and it's currently used by the unofficial Nouveau drivers for Nvidia GPUs[cite("larabelMesa20Nouveau2020")] and the official AMD ROCm drivers[cite("larabelRadeonROCmReleased2021")].

        Nvidia's Arm-based Tegra SoCs take a different approach where both the CPU and GPU use the same System MMU and DRAM[cite("nvidiacorporationNVIDIAJetsonTX22017")].
        This is also done on SoCs with Arm GPUs[cite("armltdMemoryManagementEmbedded2013")].
        On Tegra, GPUs can also snoop the CPU cache without using NVLink[cite(("nvidiacorporationS8868CUDAXavier2018", r"Slide~24"))], which they call I/O Coherency.

        The computing industry as a whole is leaning towards unifying device-attached memory with processor memory.
        CXL[cite("cxlconsortiumComputeExpressLink2021")] is a standardized protocol from Intel built on PCIe which uses three sub-protocols (CXL.io, CXL.cache, CXL.memory) to allow fine-grained coherence between device and host memory.
        # coherent device-to-processor-memory and processor-to-device-memory accesses, including allowing the device to cache processor memory.
        This is a promising protocol, and it has the backing of Nvidia, AMD, and Arm.
        It may become more relevant for GPUs in the future, but for now at least Nvidia has no reason to move away from NVLink.
        It also hasn't yet been widely implemented.
        The first Intel CPUs to support it (Sapphire Rapids microarchitecture [cite(("intelAcceleratingPossibilitiesHPC2021", r"Slide~16"))]) are due to come out in 2022 [cite("intelUpdatesIntelNextGen2021")].

        The idea of a coherent, unified address space and view of memory across all devices is compelling, especially given the convenience it affords some software.
        However, if not applied with care, it could compromise security and performance.
        If the GPU can now access the entire virtual address space for a process, it becomes an attack surface for exposing sensitive data that was previously CPU-only.
        Additionally, while a unified address space does benefit pointer-heavy compute work such as graph workloads, current graphics APIs have little to gain:
        they already abstract the concept of GPU pointers away in favor of e.g. resource IDs, and many modern game engines rely on careful control of which data is sent to the GPU and when.
        Recent graph programs [cite("wangGrusUnifiedmemoryefficientHighperformance2021","minEMOGIEfficientMemoryaccess2021")] have even reported performance issues with automatic page migration, relying on carefully configuring or entirely disabling it to regain performance.

        To be clear, low-end SoCs with shared CPU-GPU memory can benefit from unifying accesses as much as possible, because it mirrors the underlying hardware.
        However, desktop and server systems will use external GPUs with dedicated memory for a while yet, and as long as that's the case translating between the CPU-GPU domains will be important.
        This dichotomy between a unified view and a more performant, domain-aware view will remain in place for the foreseeable future, and if CHERI is going to reach widespread adoption it needs to be able to handle it.
    }
}

#-------------------------
[section("First-Year Plan")]{
    The first year of the PhD will focus on reducing the broad idea of diverging CPU/GPU capabilities to a concrete set of potential approaches.
    This problem space is bounded by two restrictions: how current drivers and programming models use pointers, and how the CPU and GPU domains are separated in hardware.
    Indeed, integrated and external GPUs may require entirely different approaches.
    On top of this, CHERI's preestablished utility for compartmentalization[cite("watsonCHERIHybridCapabilitySystem2015")] may make GPU virtualization easier.
    Recently [citeauthor("markettosPositionPaperDefending2020")] investigated using CHERI capabilities to protect DMA accesses[cite("markettosPositionPaperDefending2020")], briefly considering shared memory but not allowing capabilities to flow/be dereferenced across domains.
    My work should build on this.

    With the above in mind, I have identified four goals to achieve before writing the first-year report.
    First, I will study the flow for accessing external memory across PCIe, including address translation for virtualized GPUs, then I will see how this changes with a more unified address space provided by CXL.
    Next, I will investigate open-source drivers for Arm Mali (e.g. integrated) and Nvidia/AMD (e.g. external) GPUs, to see how they use pointers and how they implement their semi-unified address spaces.
    Finally, based on this background I will define two preliminary split CPU/GPU capability domains.
    These will serve to expose any unforeseen issues with split domains and CHERI capabilities.
    After reporting on this progress, I will start experimenting with the split domains, potentially using a graphics API model as described above.

    # TODO plan
}